{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Python + SQL data ingestion and analysis\n",
    "\n",
    "**Python:**\n",
    "\n",
    "1. Convert the CSV files into Parquet files.\n",
    "2. Move the Parquet files into Postgres as raw data tables.\n",
    "\n",
    "**SQL:**\n",
    "\n",
    "3. Create the tables.\n",
    "4. Insert data into the tables.\n",
    "5. Find the average, minimum, and maximum sensor readings for a specific sensor within a given time period.\n",
    "6. Find the sensors that experienced readings above a certain threshold value.\n",
    "7. Calculate the hourly, daily, and monthly averages for sensor readings and store the results in appropriate tables.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea48b9672e0a231a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T16:14:34.439060200Z",
     "start_time": "2023-11-06T16:14:30.715546Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def convert_csvs_to_parquet(dir_path=None):\n",
    "    \"\"\"\n",
    "    Convert all the CSV files in a folder into a Parquet file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not dir_path:\n",
    "        raise Exception(\"Please provide a path to the folder containing the CSV files\")\n",
    "\n",
    "    # Get all csv files in the folder\n",
    "    files = os.listdir(dir_path)\n",
    "    csv_files = [os.path.join(dir_path, f) for f in files if f.endswith(\".csv\")]\n",
    "\n",
    "    # Read the CSV files into a Pandas DataFrame\n",
    "    dfs = [pd.read_csv(f) for f in csv_files]\n",
    "    files_dfs_dict = dict(zip(csv_files, dfs))\n",
    "\n",
    "    # Create Parquet file for each file\n",
    "    for f, df in files_dfs_dict.items():\n",
    "        file_name = Path(f).stem + \".parquet\"\n",
    "        df.to_parquet(os.path.join(dir_path, file_name), engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "def move_parquet_into_postgre(dir_path: str):\n",
    "    \"\"\"\n",
    "    Move the parquet file into postgres\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not dir_path:\n",
    "        raise Exception(\"Please provide a path to the folder containing the CSV files\")\n",
    "\n",
    "        # Get all csv files in the folder\n",
    "    files = os.listdir(dir_path)\n",
    "    parquet_files = [os.path.join(dir_path, f) for f in files if f.endswith(\".parquet\")]    # Read the CSV files into a Pandas DataFrame\n",
    "    dfs = [pd.read_parquet(f, engine=\"pyarrow\") for f in parquet_files]\n",
    "    files_dfs_dict = dict(zip(parquet_files, dfs))\n",
    "    for f_path, df in files_dfs_dict.items():\n",
    "        df.columns = [c.lower() for c in df.columns]  # postgres doesn't like capitals or spaces\n",
    "        # Remove date and time columns\n",
    "        if 'date' in df.columns and 'time' in df.columns:\n",
    "            df = df.drop(['date', 'time'], axis=1)\n",
    "\n",
    "        table_name = Path(f_path).stem\n",
    "        engine = create_engine(f\"postgresql://postgres:123123@localhost:5432/sensorsDB\")\n",
    "\n",
    "        df.to_sql(table_name+'_raw', engine)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T16:14:39.609851700Z",
     "start_time": "2023-11-06T16:14:39.579507700Z"
    }
   },
   "id": "94a8107c43ce802c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_csvs_to_parquet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mIdan\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDataYoga\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mData\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mconvert_csvs_to_parquet\u001B[49m(dir_path\u001B[38;5;241m=\u001B[39mdata_dir)\n\u001B[0;32m      3\u001B[0m move_parquet_into_postgre(dir_path\u001B[38;5;241m=\u001B[39mdata_dir)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'convert_csvs_to_parquet' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\Idan\\DataYoga\\Data\"\n",
    "convert_csvs_to_parquet(dir_path=data_dir)\n",
    "move_parquet_into_postgre(dir_path=data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T13:39:37.235121500Z",
     "start_time": "2023-11-06T13:39:36.592138600Z"
    }
   },
   "id": "1da1a82159261620"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Create the tables :\n",
    "\n",
    "```sql\n",
    "CREATE TABLE Sensors (\n",
    "  SensorID INT NOT NULL,\n",
    "  LocationID INT NOT NULL,\n",
    "  PRIMARY KEY (SensorID)\n",
    ");\n",
    "\n",
    "CREATE TABLE Locations (\n",
    "  LocationID INT NOT NULL,\n",
    "  LocationName CHAR(256) NOT NULL,\n",
    "  PRIMARY KEY (LocationID)\n",
    ");\n",
    "\n",
    "CREATE TABLE Readings (\n",
    "  SensorID INT NOT NULL,\n",
    "  TimeStamp TIMESTAMP NOT NULL,\n",
    "  ReadingValue FLOAT NOT NULL,\n",
    "  PRIMARY KEY (SensorID, TimeStamp),\n",
    "  FOREIGN KEY (SensorID) REFERENCES Sensors (SensorID)\n",
    ");\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d14e95cd1db3f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update sensors table to have a constrain on locationID\n",
    "```sql\n",
    "ALTER TABLE Sensors\n",
    "ADD CONSTRAINT sensors_location_constraint\n",
    "FOREIGN KEY (LocationID) REFERENCES Locations (LocationID);\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06dc43542a2e462"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update Readings_raw TIMESTAMP column to be in UTC time zone.\n",
    "```sql\n",
    "ALTER TABLE Readings_raw\n",
    "ALTER COLUMN TimeStamp TYPE TIMESTAMP WITH TIME ZONE USING TimeStamp AT TIME ZONE 'UTC';\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762b77a84a41442e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Insert data into the tables\n",
    "```sql\n",
    "INSERT INTO locations (LocationID, LocationName)\n",
    "SELECT LocationID, LocationName  \n",
    "FROM public.\"Locations_raw\";\n",
    "\n",
    "INSERT INTO sensors (SensorID, LocationID)\n",
    "SELECT DISTINCT SensorID, LocationID \n",
    "FROM public.\"Sensors_raw\";\n",
    "\n",
    "INSERT INTO readings (SensorID, TimeStamp, ReadingValue)\n",
    "SELECT SensorID, CAST(TimeStamp AS timestamp with time zone), ReadingValue\n",
    "FROM public.\"Readings_raw\";\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cffa23b995f30114"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the average, minimum, and maximum sensor readings for a specific sensor within a given time period.\n",
    "```sql\n",
    "SELECT SensorID,\n",
    "       MAX(ReadingValue) AS max_reading_value,\n",
    "       MIN(ReadingValue) AS min_reading_value,\n",
    "       AVG(ReadingValue) AS average_reading_value\n",
    "FROM readings\n",
    "WHERE TimeStamp BETWEEN '2020-01-01 00:00:00' AND '2020-03-31 00:00:00'\n",
    "GROUP BY SensorID\n",
    "ORDER BY SensorID ASC;\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "495854635e66c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the sensors that experienced readings above a certain threshold value.\n",
    "```sql\n",
    "SELECT  DISTINCT SensorID as sensors_above_200\n",
    "FROM readings\n",
    "WHERE ReadingValue > 200\n",
    "ORDER BY SensorID ASC;\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a1f3507e28d1f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the hourly, daily, and monthly averages for sensor\n",
    "# readings and store the results in appropriate tables.\n",
    "\n",
    "```sql\n",
    "\n",
    "-- Create daily table\n",
    "CREATE TABLE daily_readings (\n",
    "  SensorID INT NOT NULL,\n",
    "  day INT NOT NULL,\n",
    "  avg_readings FLOAT NOT NULL,\n",
    "  PRIMARY KEY (SensorID, day)\n",
    ");\n",
    "\n",
    "-- Insert data into the table\n",
    "INSERT INTO daily_readings\n",
    "SELECT SensorID, \n",
    "\t   EXTRACT(day from TimeStamp) AS day,\n",
    "\t   AVG(ReadingValue) AS avg_readings\n",
    "FROM readings\n",
    "GROUP BY SensorID, EXTRACT(day from TimeStamp);\n",
    "\n",
    "-- Create hourly table\n",
    "CREATE TABLE hourly_readings (\n",
    "  SensorID INT NOT NULL,\n",
    "  hour INT NOT NULL,\n",
    "  avg_readings FLOAT NOT NULL,\n",
    "  PRIMARY KEY (SensorID, hour)\n",
    ");\n",
    "\n",
    "-- Insert data into the table\n",
    "INSERT INTO hourly_readings\n",
    "SELECT SensorID, \n",
    "\t   EXTRACT(hour from TimeStamp) AS hour,\n",
    "\t   AVG(ReadingValue) AS avg_readings\n",
    "FROM readings\n",
    "GROUP BY SensorID, EXTRACT(hour from TimeStamp);\n",
    "\n",
    "\n",
    "-- Create monthly table\n",
    "CREATE TABLE monthly_readings (\n",
    "  SensorID INT NOT NULL,\n",
    "  month INT NOT NULL,\n",
    "  avg_readings FLOAT NOT NULL,\n",
    "  PRIMARY KEY (SensorID, month)\n",
    ");\n",
    "\n",
    "-- Insert data into the table\n",
    "INSERT INTO monthly_readings\n",
    "SELECT SensorID, \n",
    "\t   EXTRACT(month from TimeStamp) AS month,\n",
    "\t   AVG(ReadingValue) AS avg_readings\n",
    "FROM readings\n",
    "GROUP BY SensorID, EXTRACT(month from TimeStamp);\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281f9f0e1a86854"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Trends by moving average of 8 hours\n",
    "```sql \n",
    "CREATE TABLE hourly_moving_averages (\n",
    "  sensorID INT NOT NULL,\n",
    "  hour INT NOT NULL,\n",
    "  avg_readings FLOAT NOT NULL,\n",
    "  moving_average FLOAT NOT NULL\n",
    ");\n",
    "\n",
    "INSERT INTO hourly_moving_averages\n",
    "SELECT sensorID,\n",
    "\t   hour,\n",
    "\t   avg_readings,\n",
    "       AVG(avg_readings) OVER (PARTITION BY sensorID\n",
    "                              ORDER BY sensorID, hour\n",
    "                              ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS moving_average\n",
    "FROM hourly_readings;\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238e9a7d0d12033e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
